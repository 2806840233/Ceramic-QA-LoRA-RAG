import json
import os
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM

# ======================================================
# 1. é…ç½®
# ======================================================
MODEL_PATH = "/data1/liutao/LiJin/2026/models/Qwen2.5-7B-Instruct"
VAL_FILE = "../data/CeramicQA_val.jsonl"
OUT_JSON = "CeramicQA_qwen_preds_val.json"

BATCH_SIZE = 4
MAX_NEW_TOKENS = 256

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ======================================================
# 2. åŠ è½½æ¨¡å‹
# ======================================================
print("åŠ è½½æ¨¡å‹ä¸­...")
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    device_map="auto",
    torch_dtype=torch.float16,
    trust_remote_code=True
)
model.eval()
print("æ¨¡å‹åŠ è½½å®Œæˆï¼")

# ======================================================
# 3. è¯»å–éªŒè¯é›†
# ======================================================
val_data = []
with open(VAL_FILE, "r", encoding="utf-8") as f:
    for line in f:
        val_data.append(json.loads(line))

total_num = len(val_data)
print(f"éªŒè¯é›†å¤§å°: {total_num} æ¡")

# ======================================================
# 4. è¯»å–å·²å®Œæˆçš„é¢„æµ‹ï¼ˆæ–­ç‚¹ï¼‰
# ======================================================
results = []

if os.path.exists(OUT_JSON):
    with open(OUT_JSON, "r", encoding="utf-8") as f:
        results = json.load(f)
    print(f"æ£€æµ‹åˆ°å·²æœ‰é¢„æµ‹ç»“æœ: {len(results)} æ¡ï¼Œå°†ä»æ–­ç‚¹ç»§ç»­")
else:
    print("æœªæ£€æµ‹åˆ°å·²æœ‰ç»“æœï¼Œä»å¤´å¼€å§‹é¢„æµ‹")

start_idx = len(results)

# ======================================================
# 5. Batch æ¨ç†ï¼ˆæ–­ç‚¹ç»­è·‘ï¼‰
# ======================================================
print(f"ä»ç¬¬ {start_idx} æ¡å¼€å§‹é¢„æµ‹...")

for i in tqdm(range(start_idx, total_num, BATCH_SIZE)):
    batch = val_data[i:i + BATCH_SIZE]

    prompts = []
    metas = []

    for item in batch:
        system_prompt = ""
        user_prompt = ""
        gold_answer = ""

        for m in item["messages"]:
            if m["role"] == "system":
                system_prompt = m["content"]
            elif m["role"] == "user":
                user_prompt = m["content"]
            elif m["role"] == "assistant":
                gold_answer = m["content"]

        prompt = f"ç³»ç»Ÿ: {system_prompt}\nç”¨æˆ·: {user_prompt}\nåŠ©æ‰‹:"
        prompts.append(prompt)

        metas.append({
            "question": user_prompt,
            "reference": gold_answer
        })

    inputs = tokenizer(
        prompts,
        return_tensors="pt",
        padding=True,
        truncation=True
    ).to(DEVICE)

    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False
        )

    input_len = inputs["input_ids"].shape[1]

    for j in range(len(prompts)):
        pred_text = tokenizer.decode(
            output_ids[j][input_len:],
            skip_special_tokens=True
        ).strip()

        results.append({
            "question": metas[j]["question"],
            "reference": metas[j]["reference"],
            "prediction": pred_text
        })

    # ===== ğŸ”¥ å…³é”®ï¼šæ¯ä¸ª batch ç«‹å³è½ç›˜ =====
    with open(OUT_JSON, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

print(f"\nå…¨éƒ¨é¢„æµ‹å®Œæˆï¼Œå…± {len(results)} æ¡ï¼Œç»“æœå·²ä¿å­˜åˆ° {OUT_JSON}")
